{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e3e9ac5-ecde-48ee-a9cb-d8dc9d38be18",
   "metadata": {},
   "source": [
    "# Not working yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4372ad-c926-4be0-975c-bff7759cad3e",
   "metadata": {},
   "source": [
    "This file:\n",
    "- Calculates and exports five files\n",
    "- US_Sovi_Score = a sovi analysis using the entire us outputs county score and rank\n",
    "- FEMA_Region_Sovi_Score = a sovi analysis by fema region outputs county score and rank\n",
    "- State_Sovi_Score = a sovi analysis by state for 10 states outputs county score and rank\n",
    "- county_in_state_rank = a ranking of the counties of 10 states from the us, region level, and state level analysis\n",
    "- variable_contributions = net contributions of each variable in each analysis above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182c2a8e-d090-4540-8160-d225772efa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "skipping\n"
     ]
    }
   ],
   "source": [
    "# Import modules, define directories\n",
    "%run 'spss_pca.ipynb'\n",
    "%run 'drop1_place.ipynb'\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from scipy.stats import spearmanr\n",
    "from pyhere import here\n",
    "import numpy as np\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "# import data_prep # testing\n",
    "# from spss_pca import SPSS_PCA # testing\n",
    "# from drop1_place import * # testing\n",
    "\n",
    "pd.set_option(\"chained_assignment\", None)\n",
    "\n",
    "path = {\n",
    "    \"dscr\": here(\"data\", \"scratch\"),\n",
    "    \"drpub\": here(\"data\", \"raw\", \"public\"),\n",
    "    \"drpriv\": here(\"data\", \"raw\", \"private\"),\n",
    "    \"ddpub\": here(\"data\", \"derived\", \"public\"),\n",
    "    \"ddpriv\": here(\"data\", \"derived\", \"private\"),\n",
    "    \"rfig\": here(\"results\", \"figures\"),\n",
    "    \"roth\": here(\"results\", \"other\"),\n",
    "    \"rtab\": here(\"results\", \"tables\")\n",
    "}\n",
    "\n",
    "# data_prep.db1.copy() # testing\n",
    "# counties['GEOID'] = counties.index.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836d9657-9d80-4a09-ba73-bf79f41ee04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "# counties = gpd.read_file(here(path[\"ddpub\"], \"counties.gpkg\"))\n",
    "counties = pd.read_csv(here(path[\"ddpub\"], \"counties.csv\"), dtype = {'GEOID': object})\n",
    "US_All = pd.read_csv(here(\"data\", \"raw\", \"public\", \"spielman\", \"output\", \"sovi_inputs.csv\"))\n",
    "# counties = pd.read_csv(here(\"data\", \"scratch\", \"counties_test.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930efb4e-7c0c-41f9-8770-2b3338c28b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the relevant columns\n",
    "\n",
    "# Attribute name and expected influence on vulnerability\n",
    "input_names = [['MEDAGE_ACS', 'pos', 'person', 'Median Age'],\n",
    "               ['BLACK_ACS', 'pos', 'person', 'Pop African-American (%)'],\n",
    "               ['QNATAM_ACS', 'pos', 'person', 'Pop Native American (%)'],\n",
    "               ['QASIAN_ACS', 'pos', 'person', 'Pop Asian (%)'],\n",
    "               ['QHISP_ACS', 'pos', 'person', 'Pop Hispanic (%)'],\n",
    "               ['QAGEDEP_ACS', 'pos', 'person', 'Age Dependency (%)'],\n",
    "               ['QPUNIT_ACS', 'pos', 'person', 'Persons Per Housing Unit'],\n",
    "               ['PRENTER_ACS', 'pos', 'hu', 'Rental Housing (%)'],\n",
    "               ['QNRRES_ACS', 'pos', 'person', 'Nursing Home Residents (%)'],\n",
    "               ['QFEMALE_ACS', 'pos', 'person', 'Pop Female (%)'],\n",
    "               ['QFHH_ACS', 'pos', 'hu', 'Female-Headed Households (%)'],\n",
    "               ['QUNOCCHU_ACS', 'pos', 'hu', 'Vacant Housing (%)'],\n",
    "               ['PERCAP_ALT', 'neg', 'person', 'Per-Capita Income'],\n",
    "               ['QESL_ALT', 'pos', 'person', 'English as Second Language (%)'],\n",
    "               ['QCVLUN', 'pos', 'person', 'Unemployment (%)'],\n",
    "               ['QPOVTY', 'pos', 'person', 'Poverty (%)'],\n",
    "               ['QMOHO', 'pos', 'hu', 'Mobile Homes (%)'],\n",
    "               ['QED12LES_ALT', 'pos', 'person',\n",
    "                   'Adults Completed <Grade 12 (%)'],\n",
    "               ['QFEMLBR', 'pos', 'person', 'Female Employment (%)'],\n",
    "               ['QEXTRCT_ALT', 'pos', 'person',\n",
    "                   'Extractive Sector Employment (%)'],\n",
    "               ['QSERV_ALT', 'pos', 'person', 'Service Sector Employment (%)'],\n",
    "               ['QSSBEN', 'pos', 'hu', 'Social Security Income (%)'],\n",
    "               ['QNOAUTO_ALT', 'pos', 'hu', 'No Automobile (%)'],\n",
    "               ['QFAM', 'neg', 'person', 'Children in Married Families (%)'],\n",
    "               ['QRICH200K', 'neg', 'hu', 'Annual Income >$200K (%)'],\n",
    "               ['MDGRENT_ALT', 'neg', 'hu', 'Median Rent'],\n",
    "               ['MHSEVAL_ALT', 'neg', 'hu', 'Median Home Value'],\n",
    "               ['POPDENS', 'pos', 'person', 'Population Density']]\n",
    "\n",
    "# Get attribute names\n",
    "attr_names1 = [j[0] for j in input_names] + ['GEOID']\n",
    "attr_names2 = [j[0] for j in input_names] + ['Geo_FIPS']\n",
    "\n",
    "# Select only the columns needed to compute SoVI\n",
    "counties = counties[attr_names1]\n",
    "US_All = US_All[attr_names2]\n",
    "\n",
    "counties[\"GEOID\"] = \"g\" + counties[\"GEOID\"]\n",
    "counties['stateID'] = counties.GEOID.str.slice(0, 3, 1)\n",
    "attr_names1.remove('GEOID')\n",
    "counties = counties.set_index(counties[\"GEOID\"]).sort_index()\n",
    "# sovi_input = sovi_input.set_index(sovi_input[\"GEOID\"]).drop(labels = \"GEOID\", axis = 1)\n",
    "\n",
    "US_All['stateID'] = US_All.Geo_FIPS.str.slice(0, 3, 1)\n",
    "attr_names2.remove('Geo_FIPS')\n",
    "US_All = US_All.set_index(US_All[\"Geo_FIPS\"]).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5b926f-24df-4821-9119-8fbc118e72b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [na]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "pd.options.display.max_rows = 150\n",
    "missing = pd.DataFrame({\"na\": US_All.isna().sum()})\n",
    "missing.loc[missing[\"na\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff93d89f-8e9d-4305-be19-f1a363c16461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [na]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "missing = pd.DataFrame({\"na\": counties.isna().sum()})\n",
    "missing.loc[missing[\"na\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2fb321-61c0-489f-814e-f5e8324a3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# Uncomment these two lines to do this check:\n",
    "\n",
    "# counties = counties.drop(['GEOID'], axis=1, inplace=False)\n",
    "# US_All = US_All.drop(['Geo_FIPS'], axis=1, inplace=False)\n",
    "\n",
    "# Comment them again if you want to run the rest of the script\n",
    "\n",
    "test = counties.merge(US_All, how = \"inner\", left_on = \"GEOID\", right_on = 'Geo_FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662382e7-bd7e-4b71-867b-d9b4a5269c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QFAM_x</th>\n",
       "      <th>QFAM_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [QFAM_x, QFAM_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "test[[\"QFAM_x\", \"QFAM_y\"]].loc[test.QFAM_x.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83615008-1d96-461d-ad63-1a95988314b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "# Check for missing data\n",
    "for i in test.columns:\n",
    "    x = test[i].isnull().sum()\n",
    "    if x > 0:\n",
    "        print(i, x)\n",
    "        \n",
    "# Check for infinities\n",
    "counties_num = test.select_dtypes(include=['int64','float64'])\n",
    "for i in counties_num.columns:\n",
    "    xmin = counties_num[i].min()\n",
    "    xmax = counties_num[i].max()\n",
    "    if xmin == -np.inf:\n",
    "        print(i, \"contains a negative infinity\")\n",
    "    elif xmax == np.inf:\n",
    "        print(i, \"contains a positive infinity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c2ec6cd-e9a5-4bb2-8a52-9d3b3dea4803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEDAGE_ACS      3143\n",
       "BLACK_ACS       3143\n",
       "QNATAM_ACS      3143\n",
       "QASIAN_ACS      3143\n",
       "QHISP_ACS       3143\n",
       "QAGEDEP_ACS     3143\n",
       "QPUNIT_ACS      3143\n",
       "PRENTER_ACS     3143\n",
       "QNRRES_ACS      3143\n",
       "QFEMALE_ACS     3143\n",
       "QFHH_ACS        3143\n",
       "QUNOCCHU_ACS    3143\n",
       "PERCAP_ALT      3143\n",
       "QESL_ALT        3143\n",
       "QCVLUN          3143\n",
       "QPOVTY          3143\n",
       "QMOHO           3143\n",
       "QED12LES_ALT    3143\n",
       "QFEMLBR         3143\n",
       "QEXTRCT_ALT     3143\n",
       "QSERV_ALT       3143\n",
       "QSSBEN          3143\n",
       "QNOAUTO_ALT     3143\n",
       "QFAM            3143\n",
       "QRICH200K       3143\n",
       "MDGRENT_ALT     3143\n",
       "MHSEVAL_ALT     3143\n",
       "POPDENS            1\n",
       "stateID         3143\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "counties.eq(US_All).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e18f3ce8-c512-4912-af29-44d3f1a17c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    POPDENS_x   POPDENS_y\n",
      "0   91.902357   91.834934\n",
      "1  115.236478  115.252135\n",
      "2   31.038418   31.042757\n",
      "3   36.547352   36.571872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "POPDENS    2886\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "print(test[[\"POPDENS_x\", \"POPDENS_y\"]].head(4))\n",
    "\n",
    "counties[[\"POPDENS\"]].round(0).eq(US_All[[\"POPDENS\"]].round(0)).sum()\n",
    "# Despite being approximately the same... I think it's throwing my results off\n",
    "# approximately the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78efef4-5450-4bbb-a637-1d97bc48798d",
   "metadata": {},
   "source": [
    "Issue causing inconsistent results: the land area variable comes from different sources in the two analyses; while approximately equal there were slight differences. These differences propogate into the \"POPDENS\" variable, which in turn leads to slightly different principle components, which cause inconsistent rankings.\n",
    "\n",
    "How to address:\n",
    "\n",
    "Look through social explorer documentation and identify exactly where they got their data from. Get the exact same data, and then try to find an analog in ACS data if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6c65c-0436-445e-b50b-fca6470f589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipping Signs -- looks like this comes before calculating the Z-score\n",
    "\n",
    "# To ensure that each variable contributes as expected to the final Sovi\n",
    "# Index following Eric Tate (2012?) we flip the signs of the input data.\n",
    "for name, sign, sample, hrname in input_names:\n",
    "    if sign == 'neg':\n",
    "        counties[name] = -counties[name].values\n",
    "    elif sign == 'pos':\n",
    "        pass\n",
    "    else:\n",
    "        print(\"problem in flipping signs\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf5ecb-2e56-4483-af65-be72669faca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FEMA subRegions Dict values= state ID's\n",
    "FEMA_subs = dict()\n",
    "FEMA_subs['FEMA_1'] = ['g23g33g25', 'g50', 'g09', 'g44']\n",
    "FEMA_subs['FEMA_2'] = ['g36', 'g34']\n",
    "FEMA_subs['FEMA_3'] = ['g42', 'g10', 'g11', 'g24', 'g51', 'g54']\n",
    "FEMA_subs['FEMA_4'] = ['g21', 'g47', 'g37', 'g28', 'g01', 'g13', 'g45', 'g12']\n",
    "FEMA_subs['FEMA_5'] = ['g27', 'g55', 'g26', 'g17', 'g18', 'g39']\n",
    "FEMA_subs['FEMA_6'] = ['g35', 'g48', 'g40', 'g05', 'g22']\n",
    "FEMA_subs['FEMA_7'] = ['g31', 'g19', 'g20', 'g29']\n",
    "FEMA_subs['FEMA_8'] = ['g30', 'g38', 'g56', 'g46', 'g49', 'g08']\n",
    "FEMA_subs['FEMA_9'] = ['g06', 'g32', 'g04']\n",
    "FEMA_subs['FEMA_10'] = ['g53', 'g41', 'g16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71a460-1b0e-4c53-b346-4c937a1679c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# DataFrames to hold US, FEMA region, and state level results\n",
    "####################################\n",
    "\n",
    "# Dict to hold variable loadings\n",
    "# key will be [USA, Fema_region, stateid] depending on level of analysis\n",
    "varContrib = {}\n",
    "\n",
    "# National Score\n",
    "US_Sovi_Score = pd.DataFrame(index=counties.GEOID,\n",
    "                             columns=['sovi', 'rank'])\n",
    "\n",
    "# In the FEMA_Region_Sovi_Score data frame ranks are BY FEMA REGION.\n",
    "# The data frame holds both the SOVI score and the county rank\n",
    "# This means that there should be 10 counties with rank 1 (one for each\n",
    "# FEMA Region)\n",
    "FEMA_Region_Sovi_Score = pd.DataFrame(index=counties.GEOID,\n",
    "                                      columns=['sovi', 'rank', 'fema_region'])\n",
    "\n",
    "# Create New England conglomerate of states\n",
    "# These are the FIPS codes for the states with the letter \"g\" appended\n",
    "counties.loc[counties.stateID.isin(['g23', 'g33', 'g25']), 'stateID'] = 'g23g33g25'\n",
    "\n",
    "# These are the states in the state level analysis\n",
    "stateList = ['g23g33g25', 'g36', 'g51', 'g13',\n",
    "             'g17', 'g48', 'g29', 'g46', 'g06', 'g16']\n",
    "\n",
    "# In the State_Sovi_Score data frame ranks are BY STATE.\n",
    "# The data frame holds both the SOVI score and the county rank\n",
    "# This means that there should be 10 counties with rank 1 (one for each\n",
    "# state in stateList)\n",
    "State_Sovi_Score = pd.DataFrame(\n",
    "    index=counties.index[counties.stateID.isin(stateList)],\n",
    "    columns=['sovi', 'rank', 'state_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac84b2-c76d-4498-8280-78b2cf4cd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Compute National SoVI\n",
    "#######################\n",
    "# compute SoVI\n",
    "inputData = counties.drop(['GEOID', 'stateID'], axis=1, inplace=False)\n",
    "inputData_array = inputData.values  # Convert DataFrame to NumPy array\n",
    "pca = SPSS_PCA(inputData_array, reduce=True, varimax=True)\n",
    "\n",
    "# # FOR THE PACKAGE\n",
    "# inputData_norm = (inputData - inputData.mean(axis=0)) / inputData.std(axis=0)\n",
    "\n",
    "# # fit factor analyzer with principal components and varimax rotation\n",
    "# fa = FactorAnalyzer(rotation=\"varimax\", n_factors=28, method='principal')\n",
    "# fa.fit(inputData_norm)\n",
    "\n",
    "# # get the rotated factor pattern\n",
    "# loadings = pd.DataFrame(fa.loadings_, index=inputData_norm.columns, columns=[f\"Factor{i+1}\" for i in range(28)])\n",
    "\n",
    "\n",
    "# BACK TO TRAD\n",
    "sovi_actual_us = pca.scores_rot.sum(1)\n",
    "sovi_actual_us = pd.DataFrame(\n",
    "    sovi_actual_us, index=counties.GEOID, columns=['sovi'])\n",
    "\n",
    "# rank\n",
    "sovi_actual_us['rank'] = sovi_actual_us.rank(\n",
    "    method='average', ascending=False)\n",
    "US_Sovi_Score.update(sovi_actual_us)\n",
    "\n",
    "attrib_contribution_us = pca.weights_rot.sum(1)\n",
    "\n",
    "# Generate dictionary for all net loadings by variable for US\n",
    "varContrib['USA'] = zip(attr_names1, attrib_contribution_us.tolist())\n",
    "\n",
    "# quick check of ranks max should equal number of counties in US\n",
    "try:\n",
    "    US_Sovi_Score['rank'].max() == len(counties)\n",
    "except:\n",
    "    print(\"error in ranking check\")\n",
    "    raise\n",
    "\n",
    "# cleanup\n",
    "del inputData\n",
    "# del inputData_norm\n",
    "del sovi_actual_us\n",
    "del attrib_contribution_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6e912-4b29-46cc-a1e7-7b607052fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e7afb-5d8e-4d74-8590-119cc698f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# This is for using the package\n",
    "\n",
    "# calculate eigenvectors and eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(fa.corr_)\n",
    "eigenvalues\n",
    "# sort the eigenvalues and eigenvectors in descending order\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "\n",
    "# convert to dataframes\n",
    "eigenvalues_df = pd.DataFrame({'Eigenvalue': eigenvalues}, index=inputData_norm.columns)\n",
    "eigenvalues_df['Proportion'] = eigenvalues_df['Eigenvalue'] / eigenvalues_df['Eigenvalue'].sum()\n",
    "eigenvalues_df['Cumulative Proportion'] = eigenvalues_df['Proportion'].cumsum()\n",
    "\n",
    "# display dataframes\n",
    "print(\"Eigenvalues:\")\n",
    "display(eigenvalues_df.style.format({'Eigenvalue': '{:.4f}', 'Proportion': '{:.4f}', 'Cumulative Proportion': '{:.4f}'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4657f0e-ec01-4d87-a6a1-4b3ea94916ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# FEMA REGION SOVI\n",
    "######################\n",
    "for i in FEMA_subs:\n",
    "\n",
    "    # Subset FEMA subregion\n",
    "    FEMARegionData = counties[counties['stateID'].isin(FEMA_subs[i])]\n",
    "\n",
    "    # compute SoVI\n",
    "    inputData = FEMARegionData.drop(\n",
    "        ['GEOID', 'stateID'], axis=1, inplace=False)\n",
    "    # pca = SPSS_PCA(inputData, reduce=True, varimax=True)\n",
    "    \n",
    "    #NEW\n",
    "    inputData_array = inputData.values  # Convert DataFrame to NumPy array\n",
    "    pca = SPSS_PCA(inputData_array, reduce=True, varimax=True)\n",
    "    \n",
    "    sovi_actual_fema = pca.scores_rot.sum(1)\n",
    "\n",
    "    # load into df for merge\n",
    "    sovi_actual_fema = pd.DataFrame(\n",
    "        sovi_actual_fema, index=FEMARegionData.index, columns=['sovi'])\n",
    "    # add fema region to df\n",
    "    sovi_actual_fema['fema_region'] = i\n",
    "    # rank\n",
    "    sovi_actual_fema['rank'] = sovi_actual_fema['sovi'].rank(\n",
    "        method='average', ascending=False)\n",
    "\n",
    "    FEMA_Region_Sovi_Score.update(sovi_actual_fema)\n",
    "\n",
    "    attrib_contribution_fema = pca.weights_rot.sum(1)\n",
    "\n",
    "    # Write attribute contribution output\n",
    "    # Generate dictionary for all net loadings by variable and region\n",
    "    varContrib[i] = zip(attr_names1, attrib_contribution_fema.tolist())\n",
    "\n",
    "# cleanup\n",
    "del FEMARegionData\n",
    "del inputData\n",
    "del sovi_actual_fema\n",
    "del attrib_contribution_fema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537009ac-ef5f-4bff-9ad9-f644077e19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dec5eb-5274-4927-82c3-a5f20f809eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# State Analysis\n",
    "#############################################\n",
    "for st in stateList:\n",
    "    # Subset FEMA subregion\n",
    "    stateData = counties[counties.stateID == st]\n",
    "\n",
    "    # compute SoVI\n",
    "    inputData = stateData.drop(['GEOID', 'stateID'], axis=1, inplace=False)\n",
    "    # pca = SPSS_PCA(inputData, reduce=True, varimax=True)\n",
    "    \n",
    "    # NEW:\n",
    "    inputData_array = inputData.values  # Convert DataFrame to NumPy array\n",
    "    pca = SPSS_PCA(inputData_array, reduce=True, varimax=True)\n",
    "    \n",
    "    sovi_actual = pca.scores_rot.sum(1)\n",
    "    sovi_actual = pd.DataFrame(\n",
    "        sovi_actual, index=stateData.index, columns=['sovi'])\n",
    "    sovi_actual['state_id'] = st\n",
    "    # rank w/in state\n",
    "    sovi_actual['rank'] = sovi_actual['sovi'].rank(\n",
    "        method='average', ascending=False)\n",
    "    State_Sovi_Score.update(sovi_actual)\n",
    "    attrib_contribution = pca.weights_rot.sum(1)\n",
    "    varContrib[st] = zip(attr_names1, attrib_contribution.tolist())\n",
    "\n",
    "# cleanup\n",
    "del stateData\n",
    "del inputData\n",
    "del sovi_actual\n",
    "del attrib_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934aeef-d660-47bf-a208-8e9126ef413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1f700-b39e-47c3-a3e5-2417ddf0f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Make Var Contributions Data Frame\n",
    "###################################################\n",
    "variable_contributions = pd.DataFrame(index=attr_names1)\n",
    "# for area in varContrib.iterkeys():\n",
    "for area in varContrib.keys():\n",
    "    variable_contributions[area] = [x for i, x in varContrib[area]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a8086-9997-4b9b-8b75-b1a7281480bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Ranks w/ Geographic Extent\n",
    "# For each county rank within state for US, state, and fema_region sovis\n",
    "##########################################################################\n",
    "\n",
    "county_in_state_rank = pd.DataFrame(index=State_Sovi_Score.index,\n",
    "                                    columns=['state_sovi_rank', 'fema_region_sovi_rank', 'us_sovi_rank'])\n",
    "\n",
    "for st in stateList:\n",
    "    if st == 'g23g33g25':\n",
    "        # get all counties in the three NE states and rank for us\n",
    "        st_cty_scores1 = US_Sovi_Score.loc[['g23' in s for s in US_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores2 = US_Sovi_Score.loc[['g33' in s for s in US_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores3 = US_Sovi_Score.loc[['g25' in s for s in US_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores = pd.concat([st_cty_scores1, st_cty_scores2, st_cty_scores3])\n",
    "\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'us_sovi_rank'] = st_cty_scores.rank(method='average', ascending=False)\n",
    "\n",
    "        # get all counties in state and rank for fema region\n",
    "        st_cty_scores1 = FEMA_Region_Sovi_Score.loc[['g23' in s for s in FEMA_Region_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores2 = FEMA_Region_Sovi_Score.loc[['g33' in s for s in FEMA_Region_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores3 = FEMA_Region_Sovi_Score.loc[['g25' in s for s in FEMA_Region_Sovi_Score.index], 'sovi']\n",
    "        st_cty_scores = pd.concat([st_cty_scores1, st_cty_scores2, st_cty_scores3])\n",
    "\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'fema_region_sovi_rank'] = st_cty_scores.rank(method='average', ascending=False)\n",
    "\n",
    "        # county rank in state only sovi\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'state_sovi_rank'] = State_Sovi_Score.loc[State_Sovi_Score['state_id'] == 'g23g33g25', 'rank']\n",
    "\n",
    "    else:\n",
    "        st_cty_scores = US_Sovi_Score.loc[[st in s for s in US_Sovi_Score.index], 'sovi']\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'us_sovi_rank'] = st_cty_scores.rank(method='average', ascending=False)\n",
    "        # get all counties in state and rank for fema region\n",
    "        st_cty_scores = FEMA_Region_Sovi_Score.loc[[st in s for s in FEMA_Region_Sovi_Score.index], 'sovi']\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'fema_region_sovi_rank'] = st_cty_scores.rank(method='average', ascending=False)\n",
    "\n",
    "        # county rank in state only sovi\n",
    "        st_cty_scores = State_Sovi_Score.loc[State_Sovi_Score['state_id'] == st, 'rank']\n",
    "        county_in_state_rank.loc[st_cty_scores.index, 'state_sovi_rank'] = st_cty_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340aca3a-f23c-4597-9c24-69776862d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_in_state_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ee802-f278-47d6-92d1-ae8f4c318171",
   "metadata": {},
   "source": [
    "START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291996c-a885-4f7c-9e12-00faeb0e7ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = counties.rename(columns={\"GEOID\": \"Geo_FIPS\"})\n",
    "\n",
    "#####################################################\n",
    "# Drop 1 Variable\n",
    "#####################################################\n",
    "USvarRanks = variable_contributions.USA.sort_values()\n",
    "dropLevels = USvarRanks.index\n",
    "\n",
    "#build multindex\n",
    "geoLevels = counties.Geo_FIPS\n",
    "geoLabels = []\n",
    "for _ in range(len(dropLevels)):\n",
    "    geoLabels.extend(range(len(geoLevels)))\n",
    "dropLabels = np.repeat(range(len(dropLevels)), len(geoLevels))\n",
    "\n",
    "US_Drop1_Multi_Index = pd.MultiIndex(levels=[dropLevels, geoLevels],\n",
    "                                    codes=[dropLabels, geoLabels], # was labels but was getting error, looked at documentation and switched to a parameter that sounded similar\n",
    "                                    names=['DroppedVar', 'Geo_FIPS'])\n",
    "\n",
    "US_Drop1_NetContrib = pd.DataFrame(index=dropLevels, columns=dropLevels)\n",
    "\n",
    "US_SoVI_Drop1_Score = pd.DataFrame(index=US_Drop1_Multi_Index, columns=['sovi'])\n",
    "\n",
    "\n",
    "for j in dropLevels:\n",
    "    US_dropj = counties.drop([j, 'Geo_FIPS', 'stateID'], axis=1, inplace=False)\n",
    "    # pca = SPSS_PCA(US_dropj, reduce=True, varimax=True)\n",
    "    # NEW\n",
    "    US_dropj_array = US_dropj.values  # Convert DataFrame to NumPy array\n",
    "    pca = SPSS_PCA(US_dropj_array, reduce=True, varimax=True)\n",
    "    \n",
    "    sovi_actual = pca.scores_rot.sum(1)\n",
    "    sovi_actual = pd.DataFrame(sovi_actual, index=geoLevels, columns=['sovi'])\n",
    "    US_SoVI_Drop1_Score.loc[j, 'sovi'] = sovi_actual.values\n",
    "    attrib_contribution = pd.DataFrame(data=pca.weights_rot.sum(1), index=US_dropj.columns)\n",
    "\n",
    "    attrib_contribution = attrib_contribution.transpose()\n",
    "    attrib_contribution.index = [j]\n",
    "    US_Drop1_NetContrib.loc[attrib_contribution.columns,j] = attrib_contribution.loc[j, :]\n",
    "\n",
    "\n",
    "# sort by rank order\n",
    "US_rank_order=abs(variable_contributions.USA).rank(method='average',ascending=False).sort_values().index # original rank order\n",
    "US_Drop1_NetContrib=US_Drop1_NetContrib.loc[US_rank_order] # sort rows\n",
    "US_Drop1_NetContrib=US_Drop1_NetContrib.loc[:,US_rank_order] # sort columns\n",
    "\n",
    "# ranked version of the drop 1 variable table\n",
    "US_Drop1_NetContrib_ranks=US_Drop1_NetContrib.copy()\n",
    "US_Drop1_NetContrib_ranks=US_Drop1_NetContrib_ranks.apply(lambda x: abs(x).rank(method='average',ascending=False)) # convert absolute scores to ranks\n",
    "US_Drop1_NetContrib_ranks=US_Drop1_NetContrib_ranks.loc[US_rank_order] # sort rows\n",
    "US_Drop1_NetContrib_ranks=US_Drop1_NetContrib_ranks.loc[:,US_rank_order] # sort columns\n",
    "\n",
    "######################\n",
    "# CORRELATIONS\n",
    "######################\n",
    "state_corrs = pd.DataFrame(index = stateList, columns = ['spearman_r_st_fema', 'pvalue_st_fema', 'spearman_r_st_us', 'pvalue_st_us'])\n",
    "for st in stateList:\n",
    "  if st == 'g23g33g25':\n",
    "    multi_state_data_tmp1 = county_in_state_rank.loc[['g23' in s for s in county_in_state_rank.index], ]\n",
    "    multi_state_data_tmp2 = county_in_state_rank.loc[['g25' in s for s in county_in_state_rank.index], ]\n",
    "    multi_state_data_tmp3 = county_in_state_rank.loc[['g33' in s for s in county_in_state_rank.index], ]\n",
    "    multi_state_data_tmp = pd.concat([multi_state_data_tmp1, multi_state_data_tmp2, multi_state_data_tmp3])\n",
    "    st_fema_spearman = spearmanr(multi_state_data_tmp[['state_sovi_rank', 'fema_region_sovi_rank']])\n",
    "    st_us_spearman = spearmanr(multi_state_data_tmp[['state_sovi_rank', 'us_sovi_rank']])\n",
    "    state_corrs.loc['g23g33g25', ] = [st_fema_spearman[0], st_fema_spearman[1], st_us_spearman[0], st_us_spearman[1]]\n",
    "  else:\n",
    "    st_fema_spearman = spearmanr(county_in_state_rank.loc[[st in s for s in county_in_state_rank.index], ['state_sovi_rank', 'fema_region_sovi_rank']])\n",
    "    st_us_spearman = spearmanr(county_in_state_rank.loc[[st in s for s in county_in_state_rank.index], ['state_sovi_rank', 'us_sovi_rank']])\n",
    "    state_corrs.loc[st, ] = [st_fema_spearman[0], st_fema_spearman[1], st_us_spearman[0], st_us_spearman[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1872d-003d-4072-87b9-1a4b1c120fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08f23f-8da5-45b4-be99-bd56066efa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# DROP ONE PLACE\n",
    "################\n",
    "\n",
    "# df containing county names - no need for the geometr#\n",
    "# county_names=pd.DataFrame(gpd.read_file(os.path.join(s#path,'USA_Counties_500k.shp('../data/\n",
    "county_names = pd.read_csv(here(path[\"ddpub\"], \"counties.csv\"), dtype = {'GEOID': object})\n",
    "county_names = county_names[[\"GEOID\", \"NAME\"]]\n",
    "county_names[\"GEOID\"] = \"g\" + county_names[\"GEOID\"]\n",
    "county_names\n",
    "##### State (California)\n",
    "print('\\nDrop One Place: State\\n')\n",
    "# spearman rank correlations\n",
    "ca_cors=dropCors(counties,State_Sovi_Score,'g06')\n",
    "\n",
    "# # drop run with minimum rank correlation\n",
    "# cad=ca_cors[ca_cors==min(ca_cors)].index.values[0]\n",
    "\n",
    "# # rank change table with minimum rank correlation\n",
    "# ca_rchg=rankChgTable(inputs=US_All,scores=State_Sovi_Score,obs_names=county_names,subset='g06',drop=cad,cor=True,top=10)\n",
    "\n",
    "# # rank quantile moves with minimum rank correlation\n",
    "# ca_quint_moves=rankQuantileMoves(inputs=US_All,scores=State_Sovi_Score,subset='g06',drop=cad)\n",
    "\n",
    "# # ##### FEMA 9: California and surrounding states (includes Hawaii)\n",
    "# print('Drop One Place: FEMA\\n')\n",
    "\n",
    "# f9_cors=dropCors(US_All,FEMA_Region_Sovi_Score,'FEMA_9')\n",
    "\n",
    "# # obs that decreases the correlation most when dropped\n",
    "# f9cd=f9_cors[f9_cors==min(f9_cors)].index.values[0]\n",
    "\n",
    "# f9_rchg=rankChgTable(inputs=US_All,scores=FEMA_Region_Sovi_Score,obs_names=county_names,subset='FEMA_9',drop=f9cd,cor=True,top=10)\n",
    "\n",
    "# # rank quantile moves\n",
    "# f9_quint_moves=rankQuantileMoves(inputs=US_All,scores=FEMA_Region_Sovi_Score,subset='FEMA_9',drop=f9cd)\n",
    "\n",
    "# # ### Full USA\n",
    "# print('Drop One Place: USA\\n')\n",
    "\n",
    "# us_cors=dropCors(US_All,US_Sovi_Score)\n",
    "\n",
    "# # obs that decreases the correlation most when dropped\n",
    "# uscd=us_cors[us_cors==min(us_cors)].index.values[0]\n",
    "\n",
    "# us_rchg=rankChgTable(inputs=US_All,scores=US_Sovi_Score,obs_names=county_names,drop=uscd,cor=True,top=10)\n",
    "\n",
    "# # rank quantile moves\n",
    "# us_quint_moves=rankQuantileMoves(inputs=US_All,scores=US_Sovi_Score,drop=uscd)\n",
    "# print('\\n')\n",
    "\n",
    "# # cleanup\n",
    "# del multi_state_data_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a2eef-4d61-4977-859e-38b33f314e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# OUTPUT TABLES\n",
    "#####################################################\n",
    "US_Sovi_Score.to_csv(os.path.join(outputs, 'US_Sovi_Score.csv'))\n",
    "\n",
    "# In the FEMA_Region_Sovi_Score data frame ranks are BY FEMA REGION.\n",
    "# The data frame holds both the SOVI score and the county rank\n",
    "# This means that there should be 10 counties with rank 1 (one for each\n",
    "# FEMA Region)\n",
    "FEMA_Region_Sovi_Score.to_csv(os.path.join(\n",
    "    outputs, 'FEMA_Region_Sovi_Score.csv'))\n",
    "\n",
    "# In the State_Sovi_Score data frame ranks are BY STATE.\n",
    "# The data frame holds both the SOVI score and the county rank\n",
    "# This means that there should be 10 counties with rank 1 (one for each\n",
    "# state in stateList)\n",
    "State_Sovi_Score.to_csv(os.path.join(\n",
    "    outputs, 'State_Sovi_Score.csv'))\n",
    "\n",
    "# County rank within state for US, state, and fema_region sovis\n",
    "county_in_state_rank.to_csv(os.path.join(\n",
    "    outputs, 'County_in_State_Rank.csv'))\n",
    "\n",
    "# Variable contributions for sovis at all geographic extents\n",
    "variable_contributions.to_csv(os.path.join(\n",
    "    outputs, 'variable_contributions.csv'))\n",
    "\n",
    "# Net contribution of variables after dropping a variable\n",
    "US_Drop1_NetContrib.to_csv(os.path.join(\n",
    "    outputs, 'US_Drop1_NetContrib_raw.csv'))\n",
    "\n",
    "# rank of variables after dropping a variable\n",
    "US_Drop1_NetContrib_ranks.to_csv(os.path.join(\n",
    "    outputs, 'US_Drop1_NetContrib_ranks.csv'))\n",
    "\n",
    "# Correlation of ranks\n",
    "state_corrs.to_csv(os.path.join(\n",
    "    outputs, 'state_fema_us_rank_correlations.csv'))\n",
    "\n",
    "# Drop 1 place\n",
    "ca_rchg.to_csv(os.path.join(outputs,'drop1_place_state_rank_change_ca.csv'))\n",
    "ca_quint_moves.to_csv(os.path.join(outputs,'drop1_place_state_quint_moves_ca.csv'))\n",
    "f9_rchg.to_csv(os.path.join(outputs,'drop1_place_fema_rank_change_fema9.csv'))\n",
    "f9_quint_moves.to_csv(os.path.join(outputs,'drop1_place_fema_quint_moves_fema9.csv'))\n",
    "us_rchg.to_csv(os.path.join(outputs,'drop1_place_usa_rank_change.csv'))\n",
    "us_quint_moves.to_csv(os.path.join(outputs,'drop1_place_usa_quint_moves.csv'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
